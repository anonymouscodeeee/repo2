import json
import time
import os
import pandas as pd
import numpy as np
import torch
import argparse
import tqdm
import jax.numpy as jnp
from pathlib import Path
import logging
from transformers import (
    AutoTokenizer,
    FlaxRobertaForSequenceClassification,
    FlaxAutoModelForSequenceClassification,
    AutoModelForSequenceClassification,
)
import spu.utils.distributed as ppd
import psutil
from sklearn.metrics import accuracy_score,precision_recall_fscore_support
from jax.tree_util import tree_map

from torch.utils.data import Dataset,DataLoader

class FlattenAndCast(object):
  def __call__(self, pic):
    return np.ravel(np.array(pic, dtype=jnp.float32))

import log
LOG_DIR = Path("./logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
LOGGER = log.setup_log(__name__, "./logs/msr.log")

CHECKPOINT_DIR = Path("../finetue_codebert/checkpoint/")
TOKENIZER = AutoTokenizer.from_pretrained(
    CHECKPOINT_DIR
)
PRETRAINED_MODEL_FLAX = FlaxAutoModelForSequenceClassification.from_pretrained(
    CHECKPOINT_DIR
)
PRETRAINED_MODEL_PT = AutoModelForSequenceClassification.from_pretrained(
    CHECKPOINT_DIR
).to("cuda" if torch.cuda.is_available() else "cpu")




class DetectDataset(Dataset):
    def __init__(self,df):
        self.funcs = df.input_ids.to_list()
        self.masks=df.attention_mask.to_list()
        self.labels = df.vul.to_list()

    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self,i):          
        return {
            'input_ids': self.funcs[i],           
            'labels':  self.labels[i],
            'attention_mask':self.masks[i],
        }
    
    def len(self):
        return self.__len__()
    
    def get_item(self,i):
        return self.__getitem__(i)
    
   

def run_on_plaintext(input_ids, use_gpu):
    if use_gpu:
        logits = PRETRAINED_MODEL_PT(input_ids=input_ids.to("cuda")).logits.cpu()
        return logits.argmax(-1).to(dtype=torch.bool).tolist(), logits
    else:      
        logits = PRETRAINED_MODEL_FLAX(input_ids).logits    
        return jnp.argmax(logits, axis=1).astype(bool).tolist(), logits


MODEL = FlaxRobertaForSequenceClassification(
    PRETRAINED_MODEL_FLAX.config)

def run_on_ciphertext(seq_ids, spu_model_params): 
    def text_classification(seq_ids, model_params):
        logits = MODEL(input_ids=seq_ids, params=model_params).logits    
        return logits
    # encode context the generation is conditioned
    input_ids = ppd.device("P1")(lambda x: x)(seq_ids)
    outputs = ppd.device("SPU")(
        text_classification,
    )(input_ids, spu_model_params)

    logits = ppd.get(outputs)
    preds = jnp.argmax(logits, axis=1).astype(bool).tolist()
    return preds,logits


def stat(pre_true, pre_cpu, pre_spu, time_cpu, time_spu):
    count = len(pre_cpu)
    pre_true_sub = pre_true[:count]
    acc_cpu = accuracy_score(pre_true_sub, pre_cpu)
    acc_spu = accuracy_score(pre_true_sub, pre_spu)
    speed_cpu = time_cpu / count
    speed_spu = time_spu / count
    LOGGER.info(
        f"Accuracy: cpu {acc_cpu}, spu {acc_spu} Speed: cpu {speed_cpu} s/func, spu {speed_spu} s/func"
    )


    pid = os.getpid()
    process = psutil.Process(pid)
    memory_info = process.memory_info()
    r1 = psutil.virtual_memory()
    r2 = memory_info.rss / r1.total
    print(f"total used mem {r1.percent:.2f}, mem of current process {r2:.2f}")

def tokenize(df : pd.DataFrame,tokenizer, use_gpu):
    df_tokenized = df.copy()
    input_ids = []
    attention_mask = []     

    for f in tqdm.tqdm(df.func_before):
        res = tokenizer(f,truncation=True, return_tensors="pt" if use_gpu else "jax")
        input_ids.append(res["input_ids"].tolist())
        attention_mask.append(res["attention_mask"].tolist())

    df_tokenized["input_ids"] = input_ids
    df_tokenized["attention_mask"] = attention_mask
    return df_tokenized

def numpy_collate(batch):
    input_ids = []
    attention_mask = []
    labels = []
    for item in batch:
        input_ids.append(item["input_ids"])
        attention_mask.append(item["attention_mask"])
        labels.append(item["labels"])
    return {"input_ids": jnp.array(input_ids),
            "attention_mask": jnp.array(attention_mask),
            "labels": jnp.array(labels),}
    
def tensor_collate(batch):
    input_ids = []
    attention_mask = []
    labels = []
    for item in batch:
        input_ids.append(item["input_ids"])
        attention_mask.append(item["attention_mask"])
        labels.append(item["labels"])
    return {"input_ids": torch.tensor(input_ids),
            "attention_mask": torch.tensor(attention_mask),
            "labels": torch.tensor(labels, dtype=torch.long),}

def load_data(resume_from, specified_ids, use_gpu):
    df_msr = pd.read_csv(CHECKPOINT_DIR / "msr.csv", index_col=0)
    df_test:pd.DataFrame = None
    if specified_ids is None:
        df_test = df_msr[df_msr["split"]=="test"].copy()

        df_test["len"] = [len(f) for f in df_test.func_before]
        df_test.sort_values(by=["len","index"], inplace=True) 
        df_test.reset_index(drop=False,inplace=True)
        df_test = df_test.iloc[resume_from:]   
    else:
        df_test = df_msr.loc[specified_ids].copy()
        df_test["len"] = [len(f) for f in df_test.func_before]
        df_test.sort_values(by=["len","index"], inplace=True)
        df_test.reset_index(drop=False,inplace=True)        
      
    df_test = tokenize(df_test,TOKENIZER, use_gpu) 
    labels = df_test.vul
    origin_indexes = df_test["index"].to_list()
    current_indexes=df_test.index.to_list()
    return  df_test, labels, current_indexes, origin_indexes


def get_last_comm():
    comm = None
    with open(LOG_DIR/"server.log", "r") as f:
        lines = f.readlines()
    
    for i in reversed(range(len(lines))):
        key = "total send bytes "
        pos1  = lines[i].find(key)
        if pos1 != -1:
            pos2 = lines[i].find(",", pos1)
            comm = int(lines[i][pos1+len(key):pos2-1])
            break
    
    return comm
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="distributed driver.")
    parser.add_argument("-c", "--config", default="./3pc.json")
    parser.add_argument("-r", "--resume", default=0, type=int)   
    parser.add_argument("-m", "--mode", type=str, default="spu", choices=["cpu","spu","gpu"])


    parser.add_argument("-i", "--indexes_csv", type=str, default=None) 
    parser.add_argument("-o", "--output_file", type=str, default="msr.csv") 
    parser.add_argument("-l", "--logits", type=bool, default=False) 
    args = parser.parse_args() 
    LOGGER.info(f"client pid {os.getpid()}")
    use_gpu = args.mode == "gpu"
      

    with open(args.config, "r") as file:
        conf = json.load(file)
    ppd.init(conf["nodes"], conf["devices"])
    resume_from = args.resume
    if resume_from > 0:
        LOGGER.info(f"Resume from {resume_from}.")

    specified_ids = None
    if args.indexes_csv is not None:
        df_ids = pd.read_csv(args.indexes_csv)
        specified_ids = df_ids.old_id.to_list()

    df_test, labels, current_indexes, origin_indexes = load_data(resume_from, specified_ids,use_gpu)    
    dataloader = DataLoader(dataset=DetectDataset(df=df_test),batch_size=1, collate_fn=tensor_collate if use_gpu else  numpy_collate, shuffle=False)
    
    #outputfile
    output_dir = Path("./output")
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{args.mode}_{args.output_file}"       

    LOGGER.info(f"------Run on {args.mode}")    
    if args.mode in ["cpu","gpu"]:                
        for i,batch in tqdm.tqdm(enumerate(dataloader)):             
            del batch["labels"]     
            del batch["attention_mask"]
            t1 = time.time()       
            pred,logits= run_on_plaintext(batch["input_ids"][0], use_gpu) 
            duration = time.time() - t1          
            # current_index, origin_index, pred, y_true
            line=f"{current_indexes[i]}, {origin_indexes[i]},{pred[0]},{duration},{labels[i]}, {logits.tolist()[0] if args.logits else ''}\n"
            with open(output_file, "a+") as f:   
                f.writelines([line])             
    elif args.mode  == "spu": 
        preds, durations, lens, comms=[],[],[],[]
        spu_model_params = ppd.device("SPU")._place_arguments(
        ppd.device("P2")(lambda x: x)(PRETRAINED_MODEL_FLAX.params)
        )[0][0]   
    
        for i, batch in tqdm.tqdm(enumerate(dataloader)): 
            del batch["labels"]   
            del batch["attention_mask"] 
            tokens_count = len(batch["input_ids"][0][0])
            t1 = time.time()
            pred,logits= run_on_ciphertext(batch["input_ids"][0],spu_model_params)            
            duration = time.time() - t1
            LOGGER.info(f"seq len: {tokens_count}, duration {duration}")

            #save result       
            comm = get_last_comm()
            assert comm is not None       
            line = f"{current_indexes[i]}, {origin_indexes[i]},{pred[0]},{tokens_count},{duration},{comm}, {logits.tolist()[0] if args.logits else ''}\n"
            with open(output_file, "a+") as f:
                f.writelines([line])



            
                  
            

        
         
       